---
title: "Lunar-Bench: Evaluating Task-Oriented Reasoning of LLMs in Lunar Exploration Scenarios"
date: 2025-05-20 00:00:00 +0800
selected: true
pub: "Submitted to NeurIPS"
pub_last: ' <span class="badge badge-pill badge-publication badge-success">Under Review</span>'
pub_date: "2025"

abstract: >-
  This paper introduces Lunar-Bench, the first benchmark designed to rigorously evaluate the reasoning and decision-making capabilities of large language models (LLMs) under the unique constraints of lunar exploration. Featuring 3,000 high-fidelity tasks across critical lunar operational domains, Lunar-Bench goes beyond accuracy metrics by proposing Environmental Scenario Indicators (ESI), which assess models' safety, efficiency, factual integrity, and alignment. Evaluations of 36 state-of-the-art LLMs reveal significant performance gaps compared to human experts, underscoring the urgent need for robust, domain-adapted solutions in mission-critical AI deployment.
cover: /assets/images/covers/Cover2.jpg
authors:
  - "<strong>Xin-Yu Xiao</strong>"
  - Ye Tian
  - Yalei Liu
  - Xiangyu Liu
  - Tianyang Lu
  - Erwei Yin
  - Qianchen Xia
  - Shanguang Chen 
links:
  Paper: /assets/files/Lunar_Bench.pdf
  Code: https://github.com/Xin-YuXiao/Lunar-Bench
---
